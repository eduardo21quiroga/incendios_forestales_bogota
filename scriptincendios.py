# -*- coding: utf-8 -*-
"""ScriptIncendios.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lU4CkWDpRDJae_Xfuztar0oRYEwgralc
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.tree import DecisionTreeClassifier, plot_tree
import numpy as np
from logging import log
import math as math
from sklearn.metrics import make_scorer, recall_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import classification_report, roc_curve, auc
from sklearn.model_selection import cross_val_score

data=pd.read_excel("Final Finalisimo.xlsx")

data.head()

#Contar Incendios por año

if not pd.api.types.is_datetime64_any_dtype(data['Fecha']):
  data['Fecha'] = pd.to_datetime(data['Fecha'])

data['Year'] = data['Fecha'].dt.year


fire_counts_by_year = data[data['area'] > 0].groupby('Year')['area'].count()

print(fire_counts_by_year)

#Grafica de barras de incendios

plt.figure(figsize=(10, 6))
ax = plt.gca()
fire_counts_by_year.plot(kind='bar', color='skyblue')
for p in ax.patches:
    ax.annotate(f'{p.get_height()}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='baseline', fontsize=10, color='black', xytext=(0, 5),
                textcoords='offset points')

plt.title('Número de Incendios por Año')
plt.xlabel('Año')
plt.ylabel('Número de Incendios')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

#Contar Nas
data.isna().sum()

#Eliminamos na
data=data.dropna()

data.shape

#Valores distintos de 0 en "area"
data[data["area"]!=0].shape

#Datos con NA en "Temp" y diferentes de 0 en area
data[(data["Temp"].isna()) & (data["area"]!=0)].shape

#Contar cuantas variables hay en area
data.nunique()

data.isna().sum()

#convertir los datos de area en 1 si area>0
data["area"] = data["area"].apply(lambda x: 1 if x > 0 else 0)

#contar cuantos 1 y 0 hay
data["area"].value_counts()

data=data.drop(columns=["Fecha",'Year'])

# Funciones para calcular FFMC, DMC e ISI
def calculate_ffmc(precipitation, temp, humidity_relative, wind_speed):
    mo = 147.2 * (101.0 - humidity_relative) / (59.5 + humidity_relative)
    rf = precipitation - 0.5
    W = 0.5 + (0.579 * rf if rf > 0 else 0)
    ed = 0.942 * (temp + 6.0) + 1.894 * wind_speed * (1 - 0.0345 * humidity_relative)
    return 101.0 - (59.5 * (250.0 - ed) / (250.0 + ed))

def calculate_dmc(precipitation, temp, humidity_relative, month):
    # Constantes basadas en el mes (meses de verano suelen ser más secos)
    Le = 6.2 * np.exp(0.0636 * temp)

    # Factor K de secado basado en la temperatura
    if temp <= -1.1:
        K = 0
    elif temp <= 0:
        K = 1.0
    elif temp <= 11.2:
        K = 1.894 * (temp + 1.1) / (3.52 + temp / 6.0)
    else:
        K = 2.3

    # Ajuste por precipitación (solo si es mayor a 1.5 mm)
    if precipitation > 1.5:
        P = 0.92 * precipitation - 1.27
        DMC = max(0, Le - P) * np.exp(-K)
    else:
        DMC = Le * np.exp(-K)

    return DMC

def calculate_isi(ffmc, wind_speed):
    return 0.208 * wind_speed * np.exp(0.05039 * ffmc)

# Calcular los índices y agregarlos al dataframe
data['FFMC'] = data.apply(lambda row: calculate_ffmc(row['Precipitación'], row['Temp'], row['Humedad_relativa'], row['Viento m/s']), axis=1)
data['DMC'] = data.apply(lambda row: calculate_dmc(row['Precipitación'], row['Temp'], row['Humedad_relativa'], month=6), axis=1)
data['ISI'] = data.apply(lambda row: calculate_isi(row['FFMC'], row['Viento m/s']), axis=1)

# Mostrar el dataframe con los nuevos índices
print(data)



# Definimos las variables predictoras excluyendo 'area'
X = data.drop('area', axis=1)

# Definimos la variable objetivo
y = data['area']

# Conjuntos de entrenamiento y prueba (80% entrenamiento, 20% test)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Aquí entrenamos el árbol de decisión
decision_tree = DecisionTreeClassifier(random_state=1234)
decision_tree.fit(X_train, y_train)

# Predicciones en el conjunto de prueba (pred_y)
y_pred = decision_tree.predict(X_test)
# Matriz de confusión
matriz_confusion = confusion_matrix(y_test, y_pred)
# Matriz de confusión usando ConfusionMatrixDisplay
disp = ConfusionMatrixDisplay(confusion_matrix=matriz_confusion, display_labels=decision_tree.classes_)
disp.plot(cmap='Blues')
plt.title("Matriz de Confusión")
plt.show()

#Metricas
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

sns.countplot(data=data, x='area')

"""# Balanceo de Clases"""

#Balanceo de clases
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
from collections import Counter

s=StandardScaler()
X_train_scaled = s.fit_transform(X_train)
X_test_scaled = s.transform(X_test)

from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler

ran=RandomOverSampler(sampling_strategy=0.6) # Se crean la mitad de la clase 0
X_train_resampled, y_train_resampled = ran.fit_resample(X_train, y_train)

y_train_resampled.value_counts()

#Arbol de decisión
modOver_tree=DecisionTreeClassifier(random_state=1234)
modOver_tree.fit(X_train_resampled, y_train_resampled)

predover_tree=modOver_tree.predict(X_test)

#Prediccion
y_pred_tree=modOver_tree.predict(X_test)
y_pred_tree_prob=modOver_tree.predict_proba(X_test)[:,1]
#Matriz de confusión
matriz_confusion_tree=confusion_matrix(y_test, predover_tree)
#Matriz bonita
disp_tree=ConfusionMatrixDisplay(confusion_matrix=matriz_confusion_tree, display_labels=modOver_tree.classes_)
disp_tree.plot(cmap='Blues')
plt.title("Matriz de Confusión")
plt.show()

#Metricas
from sklearn.metrics import classification_report
print(classification_report(y_test, predover_tree))

#Undersampling

run=RandomUnderSampler(sampling_strategy=1)
X_train_resampled_under, y_train_resampled_under = run.fit_resample(X_train, y_train)

y_train_resampled_under.value_counts()

"""#Árbol de decisión con Undersampling

"""

#Arbol de decision
modUnder_tree=DecisionTreeClassifier(random_state=1234)
modUnder_tree.fit(X_train_resampled_under, y_train_resampled_under)

predund_treeproba=modUnder_tree.predict_proba(X_test)
predund_tree=modUnder_tree.predict(X_test)

from sklearn.model_selection import cross_val_score
scores= cross_val_score(modUnder_tree, X_train_resampled_under, y_train_resampled_under, cv=5, scoring='recall')
scores

scores.mean()

#Prediccion
y_pred_tree_under=modUnder_tree.predict(X_test)
#Matriz de confusión
matriz_confusion_tree_under=confusion_matrix(y_test, predund_tree)
#Matriz bonita
disp_tree_under=ConfusionMatrixDisplay(confusion_matrix=matriz_confusion_tree_under, display_labels=modUnder_tree.classes_)
disp_tree_under.plot(cmap='Blues')
plt.title("Matriz de Confusión")
plt.show()

#Matriz de confusión
matriz_confusion_tree_under=confusion_matrix(y_test, predund_tree)
matriz_confusion_tree_under

#Función para las metricas
from sklearn.metrics import classification_report
print(classification_report(y_test, predund_tree))

#Gráfica del arbol
#Graficamos el arbol de decision con nombres de las comunas
plt.figure(figsize=(15, 10))
plot_tree(modOver_tree, filled=True,feature_names=X.columns, max_depth=3)
plt.show()

"""#Hiperparametros Arbol"""

#Los hiperparamtros a cambiar son
paramArbol = {
    'max_depth': [None, 5, 10, 15, 20],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 5, 10],
    'max_features': [None, 'sqrt', 'log2'],
    'criterion': ['gini', 'entropy'],
    'splitter': ['best', 'random']
}
#Configurar GridSearch
grid_search_tree = GridSearchCV(
    estimator=DecisionTreeClassifier(random_state=1234),
    param_grid=paramArbol,
    scoring='recall',
    cv=5)

grid_search_tree.fit(X_train_resampled_under, y_train_resampled_under)

#Ver los mejores hiperparámetros
print("Mejores hiperparámetros:", grid_search_tree.best_params_)

#Ver la mejor puntuación de recall
print("Mejor puntuación de recall:", grid_search_tree.best_score_)

#Mejor modelo arbol, gráfica
best_tree = DecisionTreeClassifier(**grid_search_tree.best_params_,random_state=1234)
best_tree.fit(X_train_resampled_under, y_train_resampled_under)
# Hacer predicciones con el mejor modelo
y_pred_best_tree = best_tree.predict(X_test)

#Gráfica arbol
plt.figure(figsize=(10, 10))
plot_tree(best_tree, filled=True,feature_names=X.columns)
plt.show()

#Matriz de confusión
matriz_confusion_tree_under_best=confusion_matrix(y_test, y_pred_best_tree)
matriz_confusion_tree_under_best
#Bonita
best_tree_under=ConfusionMatrixDisplay(confusion_matrix=matriz_confusion_tree_under_best, display_labels=modUnder_tree.classes_)
best_tree_under.plot(cmap='Blues')
plt.title("Matriz de Confusión")
plt.show()

#Metricas
from sklearn.metrics import classification_report
print(classification_report(y_test, best_tree.predict(X_test)))

# Hacemos predicciones de probabilidades (usando predict_proba)
y_pred_prob_tree = best_tree.predict_proba(X_test)[:, 1]  # Probabilidades de la clase 1 (incendio)

# Calcular la curva ROC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_tree)

# Calcular el AUC
roc_aucTree = auc(fpr, tpr)

# Imprimir el AUC
print(f"AUC-ROC: {roc_aucTree:.4f}")

# Graficar la curva ROC
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_aucTree:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Línea diagonal aleatoria
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Tasa de Falsos Positivos (FPR)')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
plt.title('Curva ROC')
plt.legend(loc="lower right")
plt.show()

"""## Importancia Características"""

# Obtener la importancia de las características del árbol de decisión
importances = best_tree.feature_importances_

# Crear un DataFrame para mostrar la importancia de las características
feature_names = X_train_resampled_under.columns
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})

# Ordenar las características por importancia de mayor a menor
importance_df = importance_df.sort_values('Importance', ascending=True)

# Graficar la importancia de las características
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Importancia')
plt.ylabel('Característica')
plt.title('Importancia de las Características en el Modelo de Árbol de Decisión')
plt.show()

"""# Regresión Logística"""

# prompt: Realiza el codigo para regresion logistica con el under

from sklearn.linear_model import LogisticRegression

# Arbol de decision
modUnder_logreg = LogisticRegression(C=10, penalty='l2', solver='liblinear', random_state=1234)
modUnder_logreg.fit(X_train_resampled_under, y_train_resampled_under)

predund_logreg = modUnder_logreg.predict(X_test)

# Prediccion
y_pred_logreg_under = modUnder_logreg.predict(X_test)

# Matriz de confusión
matriz_confusion_logreg_under = confusion_matrix(y_test, predund_logreg)

# Matriz bonita
disp_logreg_under = ConfusionMatrixDisplay(confusion_matrix=matriz_confusion_logreg_under, display_labels=modUnder_logreg.classes_)
disp_logreg_under.plot(cmap='Blues')
plt.title("Matriz de Confusión")
plt.show()

# Metricas
print(classification_report(y_test, predund_logreg))

# Busca los mejores hiperparametros para la regresion logistica con GridSearchCV

from sklearn.model_selection import GridSearchCV

# Definir los parámetros a ajustar
param_grid_logreg = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Parámetro de regularización
    'penalty': ['l1', 'l2'],  # Tipo de regularización
    'solver': ['liblinear'],  # Solvers compatibles con l1 y l2
    'max_iter': [100, 200, 300],  # Número máximo de iteraciones
    'multi_class': ['ovr']  # Estrategia para problemas multiclase
}
# Crear el modelo de regresión logística
logreg = LogisticRegression(random_state=1234)

# Crear el objeto GridSearchCV
grid_searchRL = GridSearchCV(
    estimator=logreg,
    param_grid=param_grid_logreg,
    scoring='recall',  # Usar recall como métrica de evaluación
    cv=5,  # Número de folds para la validación cruzada
    n_jobs=-1  # Usar todos los núcleos del procesador
)

# Ajustar el modelo con los datos de entrenamiento remuestreados
grid_searchRL.fit(X_train_resampled_under, y_train_resampled_under)

# Imprimir los mejores parámetros encontrados
print("Mejores parámetros:", grid_searchRL.best_params_)

# Imprimir la mejor puntuación de recall
print("Mejor puntuación de recall:", grid_searchRL.best_score_)

# Entrenar el modelo con los mejores parámetros
best_logreg = LogisticRegression(**grid_searchRL.best_params_, random_state=1234)
best_logreg.fit(X_train_resampled_under, y_train_resampled_under)

# Hacer predicciones con el mejor modelo
y_pred_best_logreg = best_logreg.predict(X_test)

# Evaluar el mejor modelo
print(classification_report(y_test, y_pred_best_logreg))

"""## Matriz RL"""

#Matriz de confusion
matriz_confusion_logreg_under_best=confusion_matrix(y_test, y_pred_best_logreg)
matriz_confusion_logreg_under_best
#Bonita
best_logreg_under=ConfusionMatrixDisplay(confusion_matrix=matriz_confusion_logreg_under_best, display_labels=best_logreg.classes_)
best_logreg_under.plot(cmap='Blues')
plt.title("Matriz de Confusión")

"""## AUC-ROC Regresión Logística"""

# Hacemos predicciones de probabilidades (usando predict_proba)
y_pred_prob_RL = best_logreg.predict_proba(X_test)[:, 1]  # Probabilidades de la clase 1 (incendio)

# Calcular la curva ROC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_RL)

# Calcular el AUC
roc_aucRL = auc(fpr, tpr)

# Imprimir el AUC
print(f"AUC-ROC: {roc_aucRL:.4f}")

# Graficar la curva ROC
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_aucRL:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Línea diagonal aleatoria
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Tasa de Falsos Positivos (FPR)')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
plt.title('Curva ROC')
plt.legend(loc="lower right")
plt.show()

"""# Random Forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import classification_report
import pandas as pd
from scipy.stats import randint
rf_model = RandomForestClassifier(random_state=1234)

# Definir la rejilla de hiperparámetros
param_dist_RF = {
    'n_estimators': randint(50, 200),  # Número de árboles en el bosque
    'max_depth': [None, 5, 10, 15],  # Profundidad máxima de los árboles
    'min_samples_split': randint(2, 10),  # Número mínimo de muestras para dividir un nodo
    'min_samples_leaf': randint(1, 4),  # Número mínimo de muestras en una hoja
    'bootstrap': [True, False],  # Usar bootstrapping para construir los árboles
    'criterion': ['gini', 'entropy']  # Función para medir la calidad de una división
}

# Crear el objeto RandomizedSearchCV
random_search_rf = RandomizedSearchCV(
    estimator=rf_model,
    param_distributions=param_dist_RF,
    n_iter=50,  # Número de combinaciones aleatorias a probar
    scoring='recall',
    cv=5,
    n_jobs=-1,
    random_state=1234
)

# Ajustar el modelo con los datos de entrenamiento remuestreados
random_search_rf.fit(X_train_resampled_under, y_train_resampled_under)

# Imprimir los mejores parámetros encontrados
print("Mejores parámetros:", random_search_rf.best_params_)
print("Mejor puntuación de recall:", random_search_rf.best_score_)

# Entrenar el modelo con los mejores parámetros
best_rf = RandomForestClassifier(**random_search_rf.best_params_, random_state=1234)
best_rf.fit(X_train_resampled_under, y_train_resampled_under)

# Hacer predicciones con el mejor modelo
y_pred_best_rf = best_rf.predict(X_test)

# Evaluar el mejor modelo
print(classification_report(y_test, y_pred_best_rf))

"""## Matriz RF"""

#Matriz de confusión
matriz_confusion_rf_under_best=confusion_matrix(y_test, y_pred_best_rf)
matriz_confusion_rf_under_best
#bonita
best_rf_under=ConfusionMatrixDisplay(confusion_matrix=matriz_confusion_rf_under_best, display_labels=best_rf.classes_)
best_rf_under.plot(cmap='Blues')
plt.title("Matriz de Confusión")

"""## AUC ROC"""

# Hacemos predicciones de probabilidades (usando predict_proba)
y_pred_prob_RF = best_rf.predict_proba(X_test)[:, 1]  # Probabilidades de la clase 1 (incendio)

# Calcular la curva ROC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_RF)

# Calcular el AUC
roc_aucRF = auc(fpr, tpr)

# Imprimir el AUC
print(f"AUC-ROC: {roc_aucRF:.4f}")

# Graficar la curva ROC
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_aucRF:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Línea diagonal aleatoria
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Tasa de Falsos Positivos (FPR)')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
plt.title('Curva ROC')
plt.legend(loc="lower right")
plt.show()

"""## Importancia Características"""

# Obtener la importancia de las características
importances = best_rf.feature_importances_

# Crear un DataFrame para mostrar la importancia de las características
feature_names = X_train_resampled_under.columns
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})

# Ordenar las características por importancia de mayor a menor
importance_df = importance_df.sort_values('Importance', ascending=True)

# Graficar la importancia de las características
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Importancia')
plt.ylabel('Característica')
plt.title('Importancia de las Características en el Modelo Random Forest')
plt.show()

"""#Naive Bayes"""

# prompt: Busca los mejores hiperparametros para Naive Bayes con GridSearchCV, cambia el "param_grid" por "param_grid_NV"

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import GridSearchCV

# Definir los parámetros a ajustar para Naive Bayes
param_grid_NV = {
    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6]  # Parámetro de suavizado para la varianza
}

# Crear el modelo de Naive Bayes
nb_model = GaussianNB()

# Crear el objeto GridSearchCV
grid_search_nb = GridSearchCV(
    estimator=nb_model,
    param_grid=param_grid_NV,
    scoring='recall',
    cv=5,
    n_jobs=-1
)

# Ajustar el modelo con los datos de entrenamiento remuestreados
grid_search_nb.fit(X_train_resampled_under, y_train_resampled_under)

# Imprimir los mejores parámetros encontrados
print("Mejores parámetros:", grid_search_nb.best_params_)

# Imprimir la mejor puntuación de recall
print("Mejor puntuación de recall:", grid_search_nb.best_score_)

# Entrenar el modelo con los mejores parámetros
best_nb = GaussianNB(**grid_search_nb.best_params_)
best_nb.fit(X_train_resampled_under, y_train_resampled_under)

# Hacer predicciones con el mejor modelo
y_pred_best_nb = best_nb.predict(X_test)

# Evaluar el mejor modelo
print(classification_report(y_test, y_pred_best_nb))

# Calcular y mostrar otras métricas
accuracy = accuracy_score(y_test, y_pred_best_nb)
precision = precision_score(y_test, y_pred_best_nb)
recall = recall_score(y_test, y_pred_best_nb)
f1 = f1_score(y_test, y_pred_best_nb)

print(f"Exactitud: {accuracy:.4f}")
print(f"Precisión: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")

"""## Matriz NB"""

#Matriz de confusión
matriz_confusion_nb_under_best=confusion_matrix(y_test, y_pred_best_nb)
matriz_confusion_nb_under_best
#Bonita
best_nb_under=ConfusionMatrixDisplay(confusion_matrix=matriz_confusion_nb_under_best, display_labels=best_nb.classes_)
best_nb_under.plot(cmap='Blues')
plt.title("Matriz de Confusión")

"""##AUC ROC NB

"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, roc_auc_score


# Hacemos predicciones de probabilidades (usando predict_proba)
y_pred_prob_NB = best_nb.predict_proba(X_test)[:, 1]  # Probabilidades de la clase 1 (incendio)

# Calcular la curva ROC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_NB)

# Calcular el AUC
roc_aucNB = auc(fpr, tpr)

# Imprimir el AUC
print(f"AUC-ROC: {roc_aucNB:.4f}")

# Graficar la curva ROC
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_aucNB:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Línea diagonal aleatoria
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Tasa de Falsos Positivos (FPR)')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
plt.title('Curva ROC')
plt.legend(loc="lower right")
plt.show()

"""# KNN"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

# Definir los parámetros a ajustar para KNN
param_grid_KNN = {
    'n_neighbors': [3, 5, 7, 9, 11],  # Número de vecinos
    'weights': ['uniform', 'distance'],  # Ponderación de los vecinos
    'metric': ['euclidean', 'manhattan']  # Métrica de distancia
}

# Crear el modelo de KNN
knn_model = KNeighborsClassifier()

# Crear el objeto GridSearchCV
grid_search_knn = GridSearchCV(
    estimator=knn_model,
    param_grid=param_grid_KNN,
    scoring='recall',
    cv=5,
    n_jobs=-1
)

# Ajustar el modelo con los datos de entrenamiento remuestreados
grid_search_knn.fit(X_train_resampled_under, y_train_resampled_under)

# Imprimir los mejores parámetros encontrados
print("Mejores parámetros:", grid_search_knn.best_params_)

# Imprimir la mejor puntuación de recall
print("Mejor puntuación de recall:", grid_search_knn.best_score_)

# Entrenar el modelo con los mejores parámetros
best_knn = KNeighborsClassifier(**grid_search_knn.best_params_)
best_knn.fit(X_train_resampled_under, y_train_resampled_under)

# Hacer predicciones con el mejor modelo
y_pred_best_knn = best_knn.predict(X_test)

# Evaluar el mejor modelo
print(classification_report(y_test, y_pred_best_knn))

#Comparar las metricas sin hiperparametros en KNN

from sklearn.neighbors import KNeighborsClassifier

# Crear el modelo KNN sin hiperparámetros
knn_model_default = KNeighborsClassifier()

# Entrenar el modelo con los datos de entrenamiento remuestreados
knn_model_default.fit(X_train_resampled_under, y_train_resampled_under)

# Hacer predicciones con el modelo por defecto
y_pred_knn_default = knn_model_default.predict(X_test)

# Evaluar el modelo por defecto y mostrar las métricas
print(classification_report(y_test, y_pred_knn_default))

"""## Matriz KNN"""

#Matriz de confusion  KNN
matriz_confusion_knn_under_best=confusion_matrix(y_test, y_pred_best_knn)
matriz_confusion_knn_under_best
#bonita
best_knn_under=ConfusionMatrixDisplay(confusion_matrix=matriz_confusion_knn_under_best, display_labels=best_knn.classes_)
best_knn_under.plot(cmap='Blues')
plt.title("Matriz de Confusión")

"""## AUC ROC"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, roc_auc_score

# Supongamos que ya tienes un modelo ajustado (best_knn, en este caso)
# Hacemos predicciones de probabilidades (usando predict_proba)
y_pred_prob_KNN = best_knn.predict_proba(X_test)[:, 1]  # Probabilidades de la clase 1 (incendio)

# Calcular la curva ROC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_KNN)

# Calcular el AUC
roc_aucKNN = auc(fpr, tpr)

# Imprimir el AUC
print(f"AUC-ROC: {roc_aucKNN:.4f}")

# Graficar la curva ROC
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_aucKNN:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Línea diagonal aleatoria
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Tasa de Falsos Positivos (FPR)')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
plt.title('Curva ROC')
plt.legend(loc="lower right")
plt.show()

# prompt: Dame un codigo que diga la importancia de las caracteristicas en mi X_train_resampled_under, y_train_resampled_under

# Entrenar un modelo de árbol de decisión (o Random Forest) para obtener la importancia de las características
model = DecisionTreeClassifier(random_state=1234)  # Puedes usar RandomForestClassifier en su lugar
model.fit(X_train_resampled_under, y_train_resampled_under)

# Obtener la importancia de las características
importances = model.feature_importances_

# Crear un DataFrame para mostrar la importancia de las características
feature_importances = pd.DataFrame({'Característica': X_train_resampled_under.columns, 'Importancia': importances})

# Ordenar las características por importancia de mayor a menor
feature_importances = feature_importances.sort_values('Importancia', ascending=False)

# Mostrar la importancia de las características
print(feature_importances)

# prompt: Realiza una tabla presentable con cada modelo, mostrando sus hiperparametros, sus metricas (priorizando recall) y su AUC-ROC

from tabulate import tabulate

# Crear una lista de diccionarios para cada modelo
model_data = [
    {
        "Model": "Decision Tree",
        "Hyperparameters": grid_search_tree.best_params_,
        "Recall": classification_report(y_test, y_pred_best_tree).split()[15],
        "AUC-ROC": roc_aucTree,
    },
    {
        "Model": "Logistic Regression",
        "Hyperparameters": grid_searchRL.best_params_,
        "Recall": classification_report(y_test, y_pred_best_logreg).split()[15],
        "AUC-ROC": roc_aucRL,
    },
    {
        "Model": "Random Forest",
        "Hyperparameters": random_search_rf.best_params_,
        "Recall": classification_report(y_test, y_pred_best_rf).split()[15],
        "AUC-ROC": roc_aucRF,
    },
    {
        "Model": "Naive Bayes",
        "Hyperparameters": grid_search_nb.best_params_,
        "Recall": classification_report(y_test, y_pred_best_nb).split()[15],
        "AUC-ROC": roc_aucNB,
    },
    {
        "Model": "KNN",
        "Hyperparameters": grid_search_knn.best_params_,
        "Recall": classification_report(y_test, y_pred_best_knn).split()[15],
        "AUC-ROC": round(roc_aucKNN,3),
    },
]

# Crear la tabla usando tabulate
table = tabulate(model_data, headers="keys", tablefmt="pretty")

# Imprimir la tabla
print(table)

import pandas as pd
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
from tabulate import tabulate

# Crear una lista de diccionarios para cada modelo
model_data = [
    {
        "Model": "Decision Tree",
        "Hyperparameters": grid_search_tree.best_params_,
        "Precision": round(precision_score(y_test, y_pred_best_tree), 3),
        "Recall": round(recall_score(y_test, y_pred_best_tree), 3),
        "F1-Score": round(f1_score(y_test, y_pred_best_tree), 3),
        "AUC-ROC": round(roc_aucTree, 3),
    },
    {
        "Model": "Logistic Regression",
        "Hyperparameters": grid_searchRL.best_params_,
        "Precision": round(precision_score(y_test, y_pred_best_logreg), 3),
        "Recall": round(recall_score(y_test, y_pred_best_logreg), 3),
        "F1-Score": round(f1_score(y_test, y_pred_best_logreg), 3),
        "AUC-ROC": round(roc_aucRL, 3),
    },
    {
        "Model": "Random Forest",
        "Hyperparameters": random_search_rf.best_params_,
        "Precision": round(precision_score(y_test, y_pred_best_rf), 3),
        "Recall": round(recall_score(y_test, y_pred_best_rf), 3),
        "F1-Score": round(f1_score(y_test, y_pred_best_rf), 3),
        "AUC-ROC": round(roc_aucRF, 3),
    },
    {
        "Model": "Naive Bayes",
        "Hyperparameters": grid_search_nb.best_params_,
        "Precision": round(precision_score(y_test, y_pred_best_nb), 3),
        "Recall": round(recall_score(y_test, y_pred_best_nb), 3),
        "F1-Score": round(f1_score(y_test, y_pred_best_nb), 3),
        "AUC-ROC": round(roc_aucNB, 3),
    },
    {
        "Model": "KNN",
        "Hyperparameters": grid_search_knn.best_params_,
        "Precision": round(precision_score(y_test, y_pred_best_knn), 3),
        "Recall": round(recall_score(y_test, y_pred_best_knn), 3),
        "F1-Score": round(f1_score(y_test, y_pred_best_knn), 3),
        "AUC-ROC": round(roc_aucKNN, 3),
    },
]

# Crear la tabla usando tabulate
table = tabulate(model_data, headers="keys", tablefmt="pretty")

# Imprimir la tabla
print(table)

"""#Comparación Métricas"""

# Crear una lista de diccionarios para cada modelo
model_data = [
    {
        "Model": "Decision Tree",
        "Precision": round(precision_score(y_test, y_pred_best_tree), 3),
        "Recall": round(recall_score(y_test, y_pred_best_tree), 3),
        "F1-Score": round(f1_score(y_test, y_pred_best_tree), 3),
        "AUC-ROC": round(roc_aucTree, 3),
        "Accuracy": round(accuracy_score(y_test, y_pred_best_tree), 3),
    },
    {
        "Model": "Logistic Regression",
        "Precision": round(precision_score(y_test, y_pred_best_logreg), 3),
        "Recall": round(recall_score(y_test, y_pred_best_logreg), 3),
        "F1-Score": round(f1_score(y_test, y_pred_best_logreg), 3),
        "AUC-ROC": round(roc_aucRL, 3),
        "Accuracy": round(accuracy_score(y_test, y_pred_best_logreg), 3),
    },
    {
        "Model": "Random Forest",
        "Precision": round(precision_score(y_test, y_pred_best_rf), 3),
        "Recall": round(recall_score(y_test, y_pred_best_rf), 3),
        "F1-Score": round(f1_score(y_test, y_pred_best_rf), 3),
        "AUC-ROC": round(roc_aucRF, 3),
        "Accuracy": round(accuracy_score(y_test, y_pred_best_rf), 3),
    },
    {
        "Model": "Naive Bayes",
        "Precision": round(precision_score(y_test, y_pred_best_nb), 3),
        "Recall": round(recall_score(y_test, y_pred_best_nb), 3),
        "F1-Score": round(f1_score(y_test, y_pred_best_nb), 3),
        "AUC-ROC": round(roc_aucNB, 3),
        "Accuracy": round(accuracy_score(y_test, y_pred_best_nb), 3),
    },
    {
        "Model": "KNN",
        "Precision": round(precision_score(y_test, y_pred_best_knn), 3),
        "Recall": round(recall_score(y_test, y_pred_best_knn), 3),
        "F1-Score": round(f1_score(y_test, y_pred_best_knn), 3),
        "AUC-ROC": round(roc_aucKNN, 3),
        "Accuracy": round(accuracy_score(y_test, y_pred_best_knn), 3),
    },
]

# Crear la tabla usando tabulate
table = tabulate(model_data, headers="keys", tablefmt="pretty")

# Imprimir la tabla
print(table)

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Asegúrate de que tienes todos los modelos ajustados y que puedes obtener las probabilidades
models = {
    "Regresión Logística": best_logreg,
    "Árbol de Decisión": best_tree,
    "Random Forest": best_rf,
    "Naive Bayes": best_nb,
    "KNN": best_knn,
}

plt.figure(figsize=(10, 6))

# Calcular y graficar la curva ROC para cada modelo
for model_name, model in models.items():
    y_prob = model.predict_proba(X_test)[:, 1]  # Probabilidades para la clase positiva
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    auc = roc_auc_score(y_test, y_prob)

    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {auc:.2f})')

# Línea diagonal aleatoria
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Tasa de Falsos Positivos (FPR)')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
plt.title('Comparativa de Curvas ROC')
plt.legend(loc="lower right")
plt.show()